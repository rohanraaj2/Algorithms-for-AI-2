{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using PyTorch you can either use your own Computer or [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "You need to install the [PyTorch](https://pytorch.org/) package which comes with some extra dependencies.\n",
    "\n",
    "Install the following packages for this notebook:\n",
    "- **PyTorch**\n",
    "- **torchvision**\n",
    "- **tqdm**\n",
    "- **matplotlib**\n",
    "\n",
    "If your computer is equpped with a GPU you can also install the GPU version of PyTorch. Otherwise install the CPU version, which is smaller in size and enough for the tasks of this practical.\n",
    "\n",
    "For using the GPU version you need to fullfill some prerequisites first, which are a little time consuming.\n",
    "- Make sure that your graphics card is new enough to handle the PyTorch environment. This can be checked by searching for the compute capability of your GPU and the compute capability requirements from the PyTorch module\n",
    "- Install the latest NVIDIA driver\n",
    "- Install suitable CUDA version\n",
    "- Install CudNN\n",
    "- Install PyTorch after all previous successful steps\n",
    "\n",
    "\n",
    "Using Google Colab should avoid installing the above mentioned prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Tensors\n",
    "\n",
    "# Initialize a 1d torch tensor of size (6, 1) and name it 'data'. Initialize the tensor as random normal distribution\n",
    "\n",
    "# Code here\n",
    "data = torch.randn(6, 1)\n",
    "print(\"Data tensor:\")\n",
    "print(data)\n",
    "\n",
    "\n",
    "# Convert the torch tensor to a numpy array and convert it back afterwards. Keep the variable naming and just override the variable every time\n",
    "\n",
    "# Code here\n",
    "data = data.numpy()\n",
    "print(\"\\nConverted to numpy:\")\n",
    "print(data)\n",
    "data = torch.from_numpy(data)\n",
    "print(\"\\nConverted back to tensor:\")\n",
    "print(data)\n",
    "\n",
    "\n",
    "# Tensors have a shape, a data type and are executed on some device on your computer. Find the mentioned tensor attributes and print them.\n",
    "\n",
    "# Code here\n",
    "print(\"\\nTensor attributes:\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(f\"Device: {data.device}\")\n",
    "\n",
    "\n",
    "# Slicing works the same as with numpy arrays. No need to learn a new syntax here :)\n",
    "# Try some slicing methods (i. e. the slicing methods we discussed in the first practical)\n",
    "\n",
    "# Code here\n",
    "print(\"\\nSlicing examples:\")\n",
    "print(f\"First element: {data[0]}\")\n",
    "print(f\"Last element: {data[-1]}\")\n",
    "print(f\"First 3 elements:\\n{data[:3]}\")\n",
    "print(f\"Every other element:\\n{data[::2]}\")\n",
    "\n",
    "\n",
    "###\n",
    "# Arithmetic operations\n",
    "###\n",
    "\n",
    "# Perform a matrix multiplication with two random tensors of different shape. The value initialization is of your choice.\n",
    "\n",
    "# Code here\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4, 5)\n",
    "result_matmul = torch.matmul(tensor1, tensor2)\n",
    "print(\"\\nMatrix multiplication:\")\n",
    "print(f\"Tensor1 shape: {tensor1.shape}\")\n",
    "print(f\"Tensor2 shape: {tensor2.shape}\")\n",
    "print(f\"Result shape: {result_matmul.shape}\")\n",
    "print(f\"Result:\\n{result_matmul}\")\n",
    "\n",
    "\n",
    "# Perform the hadamard (element-wise) product with two random initialized tensors.\n",
    "\n",
    "# Code here\n",
    "tensor_a = torch.randn(3, 3)\n",
    "tensor_b = torch.randn(3, 3)\n",
    "result_hadamard = tensor_a * tensor_b\n",
    "print(\"\\nHadamard (element-wise) product:\")\n",
    "print(f\"Tensor A:\\n{tensor_a}\")\n",
    "print(f\"Tensor B:\\n{tensor_b}\")\n",
    "print(f\"Element-wise product:\\n{result_hadamard}\")\n",
    "\n",
    "\n",
    "# For more useful tensor operations, plese check out their website: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Sequential and Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now build our first neural network layers and combine them into one model\n",
    "\n",
    "# First lets define an example Linear layer.\n",
    "# Initialize a Linear layer from PyTorch of dimension (in_features=16, out_features=32).\n",
    "\n",
    "# Code here\n",
    "linear_layer = nn.Linear(in_features=16, out_features=32)\n",
    "print(\"Linear layer created:\")\n",
    "print(linear_layer)\n",
    "\n",
    "\n",
    "# Print the layer attributes and print the weight of the Linear layer\n",
    "# Forward a fitting random initialized 2d tensor through the layer and print the result\n",
    "# What is the shape of the passed (forwarded) random tensor?\n",
    "\n",
    "# Code here\n",
    "print(\"\\nLayer attributes:\")\n",
    "print(f\"Weight shape: {linear_layer.weight.shape}\")\n",
    "print(f\"Bias shape: {linear_layer.bias.shape}\")\n",
    "print(f\"Weight:\\n{linear_layer.weight}\")\n",
    "\n",
    "input_tensor = torch.randn(4, 16)  # batch_size=4, in_features=16\n",
    "output = linear_layer(input_tensor)\n",
    "print(f\"\\nInput tensor shape: {input_tensor.shape}\")\n",
    "print(f\"Output tensor shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")\n",
    "\n",
    "\n",
    "# Why does it work to just call an initialized layer by initialized_layer(input)?\n",
    "# Check the source code for the Linear Layer and its parent 'Module' class here: https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
    "# Explain in your own words why the forward function of the 'Linear' class is automatically called when passing an input through the layer, i. e. initialized_layer(input)\n",
    "# Hint: Check out the class inheritance!\n",
    "\n",
    "# Your explanation here\n",
    "\"\"\"\n",
    "The Linear class inherits from the Module class, which implements the __call__ method.\n",
    "When you call an instance of a class (e.g., initialized_layer(input)), Python invokes the __call__ method.\n",
    "In PyTorch's Module class, the __call__ method internally calls the forward() method along with some \n",
    "additional functionality like hooks and other module-related operations. This is why we can simply \n",
    "call the layer like a function, and it automatically executes the forward pass defined in the Linear class.\n",
    "\"\"\"\n",
    "print(\"\\nExplanation: The __call__ method in the Module class automatically invokes forward()\")\n",
    "\n",
    "\n",
    "# Build a sequential model with some linear layers stacked after each other. The number of layers is your choice, but be careful because it could cost a lot of time\n",
    "# to pass data through the sequential model afterwards. Start e. g. with three linear layers :)\n",
    "# You are not restricted to linear layers. Experiment a little bit here!\n",
    "\n",
    "# Code here\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),  # input layer (do not change the in_features size of this layer - we need it later)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10)  # you can change the in_features of this layer but let the out_features at size 10 here - we need it layer\n",
    ")\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialized our model in the previous section\n",
    "# Lets now also use the model to pass data through it\n",
    "\n",
    "# Use the following tensor and pass it through your model from above\n",
    "# You have to 'reformat' the tensor first\n",
    "data = torch.randn(size=(5, 1, 28, 28))\n",
    "\n",
    "# Code here\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "# Reshape from (batch_size, channels, height, width) to (batch_size, features)\n",
    "data_reshaped = data.reshape(data.shape[0], -1)  # -1 automatically calculates 1*28*28=784\n",
    "print(f\"Reshaped data shape: {data_reshaped.shape}\")\n",
    "output = model(data_reshaped)\n",
    "print(f\"Model output shape: {output.shape}\")\n",
    "print(f\"Model output:\\n{output}\")\n",
    "\n",
    "\n",
    "# read the image 'mnist_9.jpg' from the downloaded folder with the 'torchvision' python package and pass it through the network\n",
    "# How does the tensor of the image looks like? Which information is in the different dimensions?\n",
    "\n",
    "# Code here\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Check if the image exists, otherwise use a random tensor as demonstration\n",
    "image_path = 'mnist_9.jpg'\n",
    "if os.path.exists(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "else:\n",
    "    print(\"Note: mnist_9.jpg not found, using random tensor for demonstration\")\n",
    "    img_tensor = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "print(f\"\\nImage tensor shape: {img_tensor.shape}\")\n",
    "print(\"Dimensions: (batch_size, channels, height, width)\")\n",
    "print(f\"  - batch_size: {img_tensor.shape[0]} (number of images)\")\n",
    "print(f\"  - channels: {img_tensor.shape[1]} (1 for grayscale, 3 for RGB)\")\n",
    "print(f\"  - height: {img_tensor.shape[2]} (image height in pixels)\")\n",
    "print(f\"  - width: {img_tensor.shape[3]} (image width in pixels)\")\n",
    "\n",
    "# Pass through the network\n",
    "img_output = model(img_tensor.reshape(img_tensor.shape[0], -1))\n",
    "print(f\"\\nPrediction output shape: {img_output.shape}\")\n",
    "print(f\"Predicted class: {torch.argmax(img_output, dim=1).item()}\")\n",
    "\n",
    "\n",
    "# visualize the image from above with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Code here\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img_tensor.squeeze().detach().numpy(), cmap='gray')\n",
    "plt.title(f'MNIST Image - Predicted: {torch.argmax(img_output, dim=1).item()}')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Neural Network Example Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only the application of your defined model\n",
    "# You can use the following method to train your model and check its accuracy. You can also use parts of the code below for the following practicals.\n",
    "# Just execute this box and it uses the predefined model from the previous task to run a training procedure. The variable name of the model must be 'model' (or change it accordingly).\n",
    "# ATTENTION: No worries if you don't understand the implementation. This is just for showing you how your defined model performs in terms of accuracy.\n",
    "\n",
    "# Refine your model multiple times and see how the different models perform in terms of accuracy.\n",
    "\n",
    "# We use the MNIST dataset to set the model\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader  , testloader\n",
    "\n",
    "\n",
    "def train_model(model, batch_size: int = 4, epochs: int = 10):\n",
    "    # we only consider the mnist train data for this example\n",
    "    train_loader, _ = load_mnist_data(root_path='./data', batch_size=batch_size)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = []\n",
    "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
    "            imgs, targets = imgs.to(device=device), targets.to(device=device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(imgs.reshape(imgs.shape[0], -1))\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = torch.max(outputs, dim=1).indices\n",
    "            accuracy = (max_outputs.detach() == targets.detach()).to(dtype=torch.float32).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "    \n",
    "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy {torch.tensor(running_accuracy).mean():.3f}')\n",
    "\n",
    "\n",
    "# Run the model training with the name of your model variable, in this case 'model'\n",
    "train_model(model=model, batch_size=4, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
