{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": "We need the Python packages for PyTorch and matplotlib for this exercise."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our test model for this practical task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the below functionality to execute your model (that you will adjust later step by step)\n",
    "# This block of code provides you the functionality to train a model. Results are printed after each epoch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    \"\"\"\n",
    "    Loads MNIST dataset into your directory.\n",
    "    You can change the root_path to point to a already existing path if you want to safe a little bit of memory :)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def train_model(model, batch_size: int = 4, epochs: int = 10):\n",
    "    # we only consider the mnist train data for this example\n",
    "    train_loader, _ = load_mnist_data(root_path='./data', batch_size=batch_size)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    iterations = 0\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = []\n",
    "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
    "            iterations += 1\n",
    "            imgs, targets = imgs.to(device=device), targets.to(device=device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(imgs.reshape(imgs.shape[0], -1))\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = torch.max(outputs, dim=1).indices\n",
    "            accuracy = (max_outputs.detach() == targets.detach()).to(dtype=torch.float32).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "    \n",
    "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy {torch.tensor(running_accuracy).mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training progress with different weight settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this model for your tests (of course you can change the architecture a little, but it should not be necessary.)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 32),  # input layer (do not change the in_features size of this layer - we need it later)\n",
    "    nn.Linear(32, 32),\n",
    "    nn.Linear(32, 10)  # you can change the in_features of this layer but let the out_features at size 10 here - we need it layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out how to change the weights of the layers from your neural network.\n",
    "# ATTTENTION: Write your code inside the \"with torch.no_grad():\" section! This is necessary for changing the weights of the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all weights and biases of your network to zero\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network with your new settings and take a look at the results\n",
    "# Run the model training\n",
    "train_model(model=model, batch_size=4, epochs=3)\n",
    "\n",
    "# What can you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all weights and biases to constant numbers (e.g. 0.5)\n",
    "# How does the training progress?\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network with your new settings and take a look at the results\n",
    "# Run the model training\n",
    "train_model(model=model, batch_size=4, epochs=3)\n",
    "\n",
    "# What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also take a look at the gradient of the output layer\n",
    "# Access the gradients at the output layer of your model and analyze them\n",
    "\n",
    "# We first input some random values\n",
    "# forward + backward\n",
    "outputs = model(torch.randn(size=(1,784)))\n",
    "loss = nn.CrossEntropyLoss()(outputs, torch.tensor([1]))\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "# Code here\n",
    "\n",
    "\n",
    "# What can you observe?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unusual weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some weights (around 50%) of every model of the model to some weird value, e. g. extremely high (> 10.0) or extremely low (< 1e-7).\n",
    "# How does the training progress? \n",
    "# Can your model also diverge instead of converge because the weights were way to high or low?\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network with your new settings and take a look at the results\n",
    "# Run the model training\n",
    "train_model(model=model, batch_size=4, epochs=5)\n",
    "\n",
    "# What can you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight initialization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now take a closer look to the sigmoid activation function.\n",
    "# Where does the sigmoid function create small gradients and where are the biggest gradients?\n",
    "\n",
    "# Explanation here\n",
    "\n",
    "\n",
    "# Now lets plot some different activation function methods\n",
    "# Use matplotlib and plot the sigmoid activation function into the plot.\n",
    "# Create 1000 sample points from x-values [-5.0, 5.0] and create y = Sigmoid(x) and plot the result. (The result should simply be the sigmoid curve)\n",
    "# You can use the Sigmoid function from PyTorch here!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Code here\n",
    "\n",
    "\n",
    "\n",
    "# Now lets plot the kaiming normal weight initialization into the plot\n",
    "# Create 1000 points (x) sampled from the kaiming_normal_ (pytorch function) and create y = Sigmoid(kaiming_normal(1000)) and plot the result into the same plot as before.\n",
    "# Use a different color for plotting the results\n",
    "\n",
    "\n",
    "# Code here\n",
    "\n",
    "\n",
    "\n",
    "# Now plot a random normal (torch.randn) weight initialization into the plot\n",
    "# Create 1000 points (x) sampled from the randn (pytorch function) and create y = Sigmoid(randn(1000)) and plot the result into the same plot as before.\n",
    "# Use a different color for plotting the results\n",
    "\n",
    "\n",
    "# Code here\n",
    "\n",
    "\n",
    "\n",
    "# Now plot a xavier_normal weight initialization into the plot\n",
    "# Create 1000 points (x) sampled from the xavier_normal_ (pytorch function) and create y = Sigmoid(xavier_normal_(1000)) and plot the result into the same plot as before.\n",
    "# Use a different color for plotting the results\n",
    "\n",
    "\n",
    "# Code here\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Which weight initialization technique is best when using sigmoid activation function?\n",
    "\n",
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be creative and test some other weight initialization techniques! - There is so much to explore!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('aai_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
