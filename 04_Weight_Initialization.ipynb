{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We need the Python packages for PyTorch and matplotlib for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our test model for this practical task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the below functionality to execute your model (that you will adjust later step by step)\n",
    "# This block of code provides you the functionality to train a model. Results are printed after each epoch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    \"\"\"\n",
    "    Loads MNIST dataset into your directory.\n",
    "    You can change the root_path to point to a already existing path if you want to safe a little bit of memory :)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def train_model(model, batch_size: int = 4, epochs: int = 10):\n",
    "    # we only consider the mnist train data for this example\n",
    "    train_loader, _ = load_mnist_data(root_path='./data', batch_size=batch_size)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    iterations = 0\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = []\n",
    "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
    "            iterations += 1\n",
    "            imgs, targets = imgs.to(device=device), targets.to(device=device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(imgs.reshape(imgs.shape[0], -1))\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = torch.max(outputs, dim=1).indices\n",
    "            accuracy = (max_outputs.detach() == targets.detach()).to(dtype=torch.float32).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "    \n",
    "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy {torch.tensor(running_accuracy).mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training progress with different weight settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this model for your tests (of course you can change the architecture a little, but it should not be necessary.)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 32),  # input layer (do not change the in_features size of this layer - we need it later)\n",
    "    nn.Linear(32, 32),\n",
    "    nn.Linear(32, 10)  # you can change the in_features of this layer but let the out_features at size 10 here - we need it layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out how to change the weights of the layers from your neural network.\n",
    "# ATTTENTION: Write your code inside the \"with torch.no_grad():\" section! This is necessary for changing the weights of the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all weights and biases of your network to zero\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Code here\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.weight.fill_(0.0)\n",
    "            layer.bias.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network with your new settings and take a look at the results\n",
    "# Run the model training\n",
    "train_model(model=model, batch_size=4, epochs=3)\n",
    "\n",
    "# What can you observe?\n",
    "\n",
    "# The model does not learn at all. Loss stays constant and accuracy remains around 0.1 (random guessing for 10 classes).\n",
    "# This is because all neurons compute the same output (symmetry problem) and receive identical gradients,\n",
    "# so they all update in the same way and remain identical throughout training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all weights and biases to constant numbers (e.g. 0.5)\n",
    "# How does the training progress?\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Code here\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.weight.fill_(0.5)\n",
    "            layer.bias.fill_(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network with your new settings and take a look at the results\n",
    "# Run the model training\n",
    "train_model(model=model, batch_size=4, epochs=3)\n",
    "\n",
    "# What can you observe?\n",
    "\n",
    "# Similar to zero weights, the model suffers from the symmetry problem. All neurons in each layer\n",
    "# have identical weights, so they compute the same outputs and receive the same gradients.\n",
    "# Training is ineffective because neurons cannot learn different features. The network essentially\n",
    "# behaves like a single neuron per layer rather than multiple neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also take a look at the gradient of the output layer\n",
    "# Access the gradients at the output layer of your model and analyze them\n",
    "\n",
    "# We first input some random values\n",
    "# forward + backward\n",
    "outputs = model(torch.randn(size=(1,784)))\n",
    "loss = nn.CrossEntropyLoss()(outputs, torch.tensor([1]))\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "# Code here\n",
    "\n",
    "for i, layer in enumerate(model):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        print(f\"Layer {i} weight gradients:\")\n",
    "        print(layer.weight.grad)\n",
    "        print(f\"Layer {i} bias gradients:\")\n",
    "        print(layer.bias.grad)\n",
    "\n",
    "        print()\n",
    "\n",
    "# What can you observe?\n",
    "\n",
    "# All gradients for weights in each row are identical because all neurons receive the same input\n",
    "# and produce the same output due to constant initialization. This confirms the symmetry problem:\n",
    "# all neurons in a layer have identical gradients and will update identically, preventing the\n",
    "# network from learning diverse features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unusual weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some weights (around 50%) of every model of the model to some weird value, e. g. extremely high (> 10.0) or extremely low (< 1e-7).\n",
    "# How does the training progress? \n",
    "# Can your model also diverge instead of converge because the weights were way to high or low?\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Code here\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            weight_shape = layer.weight.shape\n",
    "            bias_shape = layer.bias.shape\n",
    "            \n",
    "            weight_mask = torch.rand(weight_shape) < 0.5\n",
    "            bias_mask = torch.rand(bias_shape) < 0.5\n",
    "            \n",
    "            layer.weight[weight_mask] = torch.randn(weight_mask.sum()) * 100.0\n",
    "            layer.weight[~weight_mask] = torch.randn((~weight_mask).sum()) * 1e-8\n",
    "            \n",
    "            layer.bias[bias_mask] = torch.randn(bias_mask.sum()) * 100.0\n",
    "            layer.bias[~bias_mask] = torch.randn((~bias_mask).sum()) * 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network with your new settings and take a look at the results\n",
    "# Run the model training\n",
    "train_model(model=model, batch_size=4, epochs=5)\n",
    "\n",
    "# What can you observe?\n",
    "\n",
    "# The model likely experiences exploding gradients from extremely high weights (causing NaN/inf values)\n",
    "# or vanishing gradients from extremely low weights (causing no learning). The loss may diverge or remain\n",
    "# high, and accuracy stays poor. Extreme weight values lead to unstable training where outputs saturate\n",
    "# activation functions or produce numerical overflow/underflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight initialization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now take a closer look to the sigmoid activation function.\n",
    "# Where does the sigmoid function create small gradients and where are the biggest gradients?\n",
    "\n",
    "# Explanation here\n",
    "\n",
    "# The sigmoid function has the largest gradients near x=0 (around 0.25) and smallest gradients at extreme values (x >> 0 or x << 0)\n",
    "# This is because sigmoid derivative is sigma(x) * (1 - sigma(x)), which peaks at x=0\n",
    "\n",
    "\n",
    "# Now lets plot some different activation function methods\n",
    "# Use matplotlib and plot the sigmoid activation function into the plot.\n",
    "# Create 1000 sample points from x-values [-5.0, 5.0] and create y = Sigmoid(x) and plot the result. (The result should simply be the sigmoid curve)\n",
    "# You can use the Sigmoid function from PyTorch here!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Code here\n",
    "\n",
    "x = torch.linspace(-5.0, 5.0, 1000)\n",
    "y_sigmoid = torch.sigmoid(x)\n",
    "plt.plot(x.numpy(), y_sigmoid.numpy(), label='Sigmoid curve', color='blue')\n",
    "\n",
    "\n",
    "# Now lets plot the kaiming normal weight initialization into the plot\n",
    "# Create 1000 points (x) sampled from the kaiming_normal_ (pytorch function) and create y = Sigmoid(kaiming_normal(1000)) and plot the result into the same plot as before.\n",
    "# Use a different color for plotting the results\n",
    "\n",
    "\n",
    "# Code here\n",
    "\n",
    "kaiming_weights = torch.empty(1000)\n",
    "nn.init.kaiming_normal_(kaiming_weights)\n",
    "kaiming_weights_sorted = torch.sort(kaiming_weights)[0]\n",
    "y_kaiming = torch.sigmoid(kaiming_weights_sorted)\n",
    "plt.scatter(kaiming_weights_sorted.numpy(), y_kaiming.numpy(), label='Kaiming Normal', color='red', alpha=0.5, s=5)\n",
    "\n",
    "\n",
    "# Now plot a random normal (torch.randn) weight initialization into the plot\n",
    "# Create 1000 points (x) sampled from the randn (pytorch function) and create y = Sigmoid(randn(1000)) and plot the result into the same plot as before.\n",
    "# Use a different color for plotting the results\n",
    "\n",
    "\n",
    "# Code here\n",
    "\n",
    "randn_weights = torch.randn(1000)\n",
    "randn_weights_sorted = torch.sort(randn_weights)[0]\n",
    "y_randn = torch.sigmoid(randn_weights_sorted)\n",
    "plt.scatter(randn_weights_sorted.numpy(), y_randn.numpy(), label='Random Normal', color='green', alpha=0.5, s=5)\n",
    "\n",
    "\n",
    "# Now plot a xavier_normal weight initialization into the plot\n",
    "# Create 1000 points (x) sampled from the xavier_normal_ (pytorch function) and create y = Sigmoid(xavier_normal_(1000)) and plot the result into the same plot as before.\n",
    "# Use a different color for plotting the results\n",
    "\n",
    "\n",
    "# Code here\n",
    "xavier_weights = torch.empty(1000)\n",
    "nn.init.xavier_normal_(xavier_weights)\n",
    "xavier_weights_sorted = torch.sort(xavier_weights)[0]\n",
    "y_xavier = torch.sigmoid(xavier_weights_sorted)\n",
    "plt.scatter(xavier_weights_sorted.numpy(), y_xavier.numpy(), label='Xavier Normal', color='orange', alpha=0.5, s=5)\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Which weight initialization technique is best when using sigmoid activation function?\n",
    "\n",
    "# Answer here\n",
    "\n",
    "# Xavier (Glorot) initialization is best for sigmoid activation functions because it keeps the variance of activations\n",
    "# and gradients stable across layers. It's specifically designed for activation functions with gradients centered around 0.\n",
    "# Xavier keeps weights in the linear region of sigmoid where gradients are strongest, preventing vanishing gradients.\n",
    "# Kaiming is designed for ReLU activations, while random normal can cause gradient issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be creative and test some other weight initialization techniques! - There is so much to explore!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('aai_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
