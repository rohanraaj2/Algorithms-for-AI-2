{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This task is about implementing the backpropagation algorithm.\n",
    "We will use the Stochastic Gradient Descent optimizer for optimizing the weights of a custom neural network.\n",
    "\n",
    "You can use numpy or torch for creating tensors, but not for the backpropagation (e.g. loss.backward() )!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "##################################\n",
    "# For matrices or arbitrary size #\n",
    "##################################\n",
    "class MyWeightTensor:\n",
    "    def __init__(self, shape: Tuple or int, init_weight_fn: Callable = np.random.randn, init_weights: 'MyWeightTensor' or np.ndarray or int or float = None):\n",
    "        assert isinstance(shape, tuple) or isinstance(shape, int) or isinstance(shape, float), f'Allowed shapes: tuple, int, float, got: {type(shape)}'\n",
    "        self.shape = shape\n",
    "\n",
    "        if init_weights is not None:\n",
    "            if isinstance(init_weights, MyWeightTensor):\n",
    "                self.values = init_weights.values\n",
    "            else:\n",
    "                if isinstance(shape, tuple):\n",
    "                    assert isinstance(init_weights, np.ndarray)\n",
    "                else:\n",
    "                    assert isinstance(init_weights, int) or isinstance(init_weights, float)\n",
    "                \n",
    "                self.values = init_weights\n",
    "        else:\n",
    "            if isinstance(shape, int):\n",
    "                self.shape = (self.shape,)\n",
    "                self.values = init_weight_fn(shape)\n",
    "            else:\n",
    "                self.values = init_weight_fn(*shape)\n",
    "    \n",
    "    @property\n",
    "    def T(self) -> 'MyWeightTensor':\n",
    "        _T = self.values.T\n",
    "        return MyWeightTensor(shape=_T.shape, init_weights=_T)\n",
    "    \n",
    "    def __add__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        return MyWeightTensor(shape=self.values.shape, init_weights=self.values + other)\n",
    "\n",
    "    def __mul__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        _dot = np.dot(self.values, other)\n",
    "\n",
    "        return MyWeightTensor(shape=_dot.shape, init_weights=_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearLayer:\n",
    "    def __init__(self, in_features: int, out_features: int, init_weight_fn: Callable = np.random.randn) -> None:\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = MyWeightTensor(shape=(out_features, in_features), init_weight_fn=init_weight_fn)\n",
    "        self.bias = MyWeightTensor(shape=out_features, init_weight_fn=init_weight_fn)\n",
    "\n",
    "        self.latest_input = None\n",
    "        self.latest_output = None\n",
    "\n",
    "    def __call__(self, tensor: np.ndarray or MyWeightTensor) -> MyWeightTensor:\n",
    "        self.latest_input = tensor\n",
    "\n",
    "        bs = -1\n",
    "        if len(tensor.shape) == 2:\n",
    "            # batch size included\n",
    "            bs = tensor.shape[0]\n",
    "            _w = self.weights * tensor.T\n",
    "        else:\n",
    "            _w = self.weights * tensor\n",
    "        \n",
    "        _bias = self.bias.values\n",
    "        if bs != -1:\n",
    "            _bias = np.tile(_bias, bs).reshape(bs, -1)\n",
    "        \n",
    "        self.latest_output = (_w + _bias.T).T\n",
    "\n",
    "        return MyWeightTensor(shape=self.latest_output.shape, init_weights=self.latest_output)\n",
    "    \n",
    "    def derivative(self) -> float:\n",
    "        assert self.latest_output is not None, 'Cannot calculate grad without a single forward pass.'\n",
    "        # Return a linear activation derivation\n",
    "        # your code\n",
    "        return 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_normal_init(*shape) -> np.ndarray:\n",
    "    assert len(shape) <= 2, 'Can only init max 2d tensors'\n",
    "    fan_in = shape[0]\n",
    "    if len(shape) == 1:\n",
    "        fan_out = fan_in\n",
    "    else:\n",
    "        fan_out = shape[1]\n",
    "    gain = 1.0\n",
    "\n",
    "    std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return np.random.normal(loc=0.0, scale=std, size=shape)\n",
    "\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self) -> None:\n",
    "        # init_weight_fn = lambda *shape: np.random.randn(*shape) / 10\n",
    "        init_weight_fn = lambda *shape: xavier_normal_init(*shape)\n",
    "        \n",
    "        # Build you own neural network with list of MyLinearLayer \n",
    "        # Note numbers of input and final output features of the neural network\n",
    "        # your code\n",
    "        self.layers = [\n",
    "            MyLinearLayer(784, 128, init_weight_fn),\n",
    "            MyLinearLayer(128, 64, init_weight_fn),\n",
    "            MyLinearLayer(64, 10, init_weight_fn)\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, tensor: np.ndarray) -> Any:\n",
    "        x = tensor\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your loss function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, predictions: MyWeightTensor or np.ndarray, targets: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes cross entropy between targets snd predictions.    \n",
    "        Returns: List of cross entropy losses (batch-wise)\n",
    "        \"\"\"\n",
    "        if isinstance(predictions, MyWeightTensor):\n",
    "            predictions = predictions.values\n",
    "        \n",
    "        if isinstance(targets, MyWeightTensor):\n",
    "            targets = targets.values\n",
    "\n",
    "        assert predictions.shape[0] == targets.shape[0]\n",
    "        if len(targets.shape) == 2:\n",
    "            targets = targets.reshape(-1)\n",
    "        predictions = torch.as_tensor(predictions)\n",
    "        targets = torch.as_tensor(targets)\n",
    "\n",
    "        loss = np.array([F.cross_entropy(pred, t).item() for pred, t in zip(predictions, targets)])\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def derivative(self) -> Callable:\n",
    "        # y_hat is the prediction\n",
    "        # y is the target value\n",
    "        def _derivative(y_hat: MyWeightTensor or np.ndarray, y: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
    "            if isinstance(y_hat, MyWeightTensor):\n",
    "                y_hat = y_hat.values\n",
    "            \n",
    "            if isinstance(y, MyWeightTensor):\n",
    "                y = y.values\n",
    "\n",
    "            # implement the derivation of the cross entropy \n",
    "            # your code\n",
    "            batch_size = y_hat.shape[0]\n",
    "            grad = np.zeros_like(y_hat)\n",
    "            \n",
    "            exp_scores = np.exp(y_hat - np.max(y_hat, axis=1, keepdims=True))\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "            \n",
    "            grad = probs.copy()\n",
    "            grad[np.arange(batch_size), y.flatten().astype(int)] -= 1\n",
    "            grad /= batch_size\n",
    "            \n",
    "            return grad\n",
    "        \n",
    "        return _derivative   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MyNeuralNetwork, batch_size: int, learning_rate: float, loss_fn: Callable, epochs: int = 10):\n",
    "    train_loader, _ = load_mnist_data(batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = []\n",
    "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
    "\n",
    "            # for custom model\n",
    "            imgs = imgs.numpy()\n",
    "            targets = targets.numpy()\n",
    "\n",
    "            if len(targets.shape) == 1:\n",
    "                targets = targets.reshape(-1, 1)\n",
    "\n",
    "            imgs = imgs.reshape(-1, 28 * 28)\n",
    "\n",
    "            imgs = MyWeightTensor(shape=imgs.shape, init_weights=imgs)\n",
    "\n",
    "            outputs = model(imgs).values\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            avg_loss = np.mean(loss)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += avg_loss\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = np.argmax(outputs, axis=1)\n",
    "            accuracy = (max_outputs == targets.flatten()).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "\n",
    "            #########################\n",
    "            # Start backpropagation #\n",
    "            #########################\n",
    "\n",
    "            # Your code for backpropagation!\n",
    "            \n",
    "            # Step 1: starting from the very end with loss function\n",
    "            # beginning with gradients of the loss\n",
    "            loss_derivative = loss_fn.derivative()\n",
    "            d_loss = loss_derivative(outputs, targets)\n",
    "            \n",
    "            grad_output = d_loss\n",
    "            \n",
    "            # Step 2: start back propagating the neural network \n",
    "            # for each layer calculate the output gradient and update weights and bias\n",
    "            for i in range(len(model.layers) - 1, -1, -1):\n",
    "                layer = model.layers[i]\n",
    "                \n",
    "                if isinstance(layer.latest_input, MyWeightTensor):\n",
    "                    input_values = layer.latest_input.values\n",
    "                else:\n",
    "                    input_values = layer.latest_input\n",
    "                \n",
    "                grad_weights = np.dot(grad_output.T, input_values)\n",
    "                grad_bias = np.sum(grad_output, axis=0)\n",
    "                \n",
    "                if i > 0:\n",
    "                    grad_output = np.dot(grad_output, layer.weights.values)\n",
    "                \n",
    "                layer.weights.values -= learning_rate * grad_weights\n",
    "                layer.bias.values -= learning_rate * grad_bias\n",
    "\n",
    "\n",
    "            #######################\n",
    "            # End backpropagation #\n",
    "            #######################\n",
    "\n",
    "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy: {torch.tensor(running_accuracy).mean():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Execute the training loop #\n",
    "#############################\n",
    "model = MyNeuralNetwork()\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('aai_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
