{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a custom generated dataset for regularization to shwo the effect to regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Execute the dataset cells. You don't have to change anything here. This is simply for generating the dataset that we use for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper code below to generate the data\n",
    "\n",
    "# Defines the number of data points to generate\n",
    "num_points = 100\n",
    "\n",
    "# Generate predictor points (x) between 0 and 5\n",
    "x = np.linspace(0, 5, num_points)\n",
    "\n",
    "# Generate the response variable (y) using the predictor points\n",
    "y = x * np.sin(x) + np.cos(x) + np.random.normal(loc=0, scale=1, size=num_points)\n",
    "\n",
    "# Generate data of the true function y = x*sin(x) \n",
    "# x_b will be used for all predictions below \n",
    "x_b = np.linspace(0, 5, 100)\n",
    "y_b = x_b * np.sin(x_b) + np.cos(x_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets with .33 and random_state = 42\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "x_train = np.reshape(x_train, newshape=(-1, 1)).astype(np.float32)\n",
    "x_test = np.reshape(x_test, newshape=(-1, 1)).astype(np.float32)\n",
    "y_train = np.reshape(y_train, newshape=(-1, 1)).astype(np.float32)\n",
    "y_test = np.reshape(y_test, newshape=(-1, 1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "\n",
    "# Plot the train data\n",
    "plt.plot(x_train,y_train, '.', label='Train data', markersize=15, color='#FF9A98')\n",
    "\n",
    "# Plot the test data\n",
    "plt.plot(x_test,y_test, '.', label='Test data', markersize=15, color='#75B594')\n",
    "\n",
    "# Plot the true data\n",
    "plt.plot(x_b, y_b, '-', label='True function', linewidth=3, color='#5E5E5E')\n",
    "\n",
    "# Set the axes labels\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a neural network (sequential model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create a model that can overfit to the provided data (appropriate large in size - e. g. around 50k weights with only linear layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roessle\\Anaconda3\\envs\\aai_2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 128),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use the training and evaluation methods for your model(s)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_model(\n",
    "    model, optimizer, loss_fn, x_dataset, y_dataset, epochs, batch_size\n",
    "):\n",
    "    data_loader = DataLoader(dataset=TensorDataset(torch.as_tensor(x_dataset), torch.as_tensor(y_dataset)), batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(0, epochs):\n",
    "        for data in data_loader:\n",
    "            x, y = data\n",
    "            output = model(x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch}; Loss: {loss}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_model(\n",
    "    model, x_train, y_train, x_test\n",
    "):\n",
    "    x_train = torch.as_tensor(x_train)\n",
    "    y_train = torch.as_tensor(y_train)\n",
    "\n",
    "    x_test = torch.as_tensor(x_test)\n",
    "    # y_test = torch.as_tensor(y_test)\n",
    "    \n",
    "    train_output = model(x_train).detach()\n",
    "    test_output = model(x_test).detach()\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "\n",
    "    # Plot the true train data\n",
    "    plt.plot(x_train, y_train, '.', label='Train true data', markersize=15, color='#000000')\n",
    "\n",
    "    # Plot the predicted train data\n",
    "    plt.plot(x_train, train_output, '.', label='Train predictions', markersize=15, color='#FF9A98')\n",
    "\n",
    "    # Plot the test predictions\n",
    "    plt.plot(x_test, test_output, '.', label='Test predictions', markersize=15, color='#75B594')\n",
    "\n",
    "    # Plot the true data\n",
    "    plt.plot(x_b, y_b, '-', label='True function', linewidth=3, color='#5E5E5E')\n",
    "\n",
    "    # Set the axes labels\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize an unregularized network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your model\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create your model\n",
    "unregularized_model = nn.Sequential(\n",
    "    nn.Linear(1, 128),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "print(unregularized_model)\n",
    "\n",
    "# Init the adam optimizer\n",
    "optimizer = optim.Adam(unregularized_model.parameters(), lr=0.01)\n",
    "# Define MSE loss as loss function\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a batch size of 10\n",
    "# use 1500 epochs for training\n",
    "\n",
    "# use the train_model method to train the model\n",
    "unregularized_model = train_model(\n",
    "    model=unregularized_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    x_dataset=x_train,\n",
    "    y_dataset=y_train,\n",
    "    epochs=1500,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "# use the eval_model method to plot the results\n",
    "eval_model(unregularized_model, x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a neural network with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your model\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create your model\n",
    "dropout_model = nn.Sequential(\n",
    "    nn.Linear(1, 128),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "print(dropout_model)\n",
    "\n",
    "# Init the adam optimizer\n",
    "optimizer_dropout = optim.Adam(dropout_model.parameters(), lr=0.01)\n",
    "# Define MSE loss as loss function\n",
    "loss_fn_dropout = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a batch size of 10\n",
    "# use 1500 epochs\n",
    "\n",
    "# use the train_model method to train the model\n",
    "dropout_model = train_model(\n",
    "    model=dropout_model,\n",
    "    optimizer=optimizer_dropout,\n",
    "    loss_fn=loss_fn_dropout,\n",
    "    x_dataset=x_train,\n",
    "    y_dataset=y_train,\n",
    "    epochs=1500,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "dropout_model.eval()\n",
    "# use the eval_model method to plot the results\n",
    "eval_model(dropout_model, x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a model with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your model\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create your model\n",
    "l2_model = nn.Sequential(\n",
    "    nn.Linear(1, 128),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "print(l2_model)\n",
    "\n",
    "# Init the adam optimizer\n",
    "    # L2 regularization can be implemented by a parameter of the optimizer (no need to do it manually)\n",
    "optimizer_l2 = optim.Adam(l2_model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "# Define MSE loss as loss function\n",
    "loss_fn_l2 = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a batch size of 10\n",
    "# use 1500 epochs\n",
    "\n",
    "# use the train_model method to train the model\n",
    "l2_model = train_model(\n",
    "    model=l2_model,\n",
    "    optimizer=optimizer_l2,\n",
    "    loss_fn=loss_fn_l2,\n",
    "    x_dataset=x_train,\n",
    "    y_dataset=y_train,\n",
    "    epochs=1500,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "# use the eval_model method to plot the results\n",
    "eval_model(l2_model, x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a model with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your model\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create your model\n",
    "l1_model = nn.Sequential(\n",
    "    nn.Linear(1, 128),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "print(l1_model)\n",
    "\n",
    "\n",
    "# Init the adam optimizer\n",
    "    # L1 regularization has to be implemented manually. There is no pre-defined function to use.\n",
    "optimizer_l1 = optim.Adam(l1_model.parameters(), lr=0.01)\n",
    "# Define MSE loss as loss function and add the l1 regularization\n",
    "    # You have to access all the weights from the network (see previous exercises on how to do this)\n",
    "l1_lambda = 0.001\n",
    "\n",
    "def loss_fn_l1(output, target):\n",
    "    mse_loss = nn.MSELoss()(output, target)\n",
    "    l1_reg = torch.tensor(0., requires_grad=True)\n",
    "    for param in l1_model.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    return mse_loss + l1_lambda * l1_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a batch size of 10\n",
    "# use 1500 epochs\n",
    "\n",
    "# use the train_model method to train the model\n",
    "l1_model = train_model(\n",
    "    model=l1_model,\n",
    "    optimizer=optimizer_l1,\n",
    "    loss_fn=loss_fn_l1,\n",
    "    x_dataset=x_train,\n",
    "    y_dataset=y_train,\n",
    "    epochs=1500,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "# use the eval_model method to plot the results\n",
    "eval_model(l1_model, x_train, y_train, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('aai_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
